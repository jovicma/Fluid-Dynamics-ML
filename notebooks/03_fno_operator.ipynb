{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 03 — FNO como operador\n",
        "\n",
        "Exploração da capacidade do FNO em generalizar para novas condições iniciais do conjunto gerado."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "PROJECT_ROOT = Path('..').resolve()\n",
        "SRC_PATH = PROJECT_ROOT / 'src'\n",
        "if str(SRC_PATH) not in sys.path:\n",
        "    sys.path.insert(0, str(SRC_PATH))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "import numpy as np\n",
        "import torch\n",
        "from omegaconf import OmegaConf\n",
        "\n",
        "from riemann_ml.eval.metrics import relative_l2\n",
        "from riemann_ml.ml.fno.dataset import RiemannH5Dataset, _read_metadata\n",
        "from riemann_ml.ml.fno.model import FNO1DModel\n",
        "\n",
        "CONFIG_PATH = Path('src/riemann_ml/configs/fno.yaml')\n",
        "CHECKPOINT_DIR = Path('data/artifacts/fno/checkpoints')\n",
        "DATASET_PATH = Path('data/processed/sod_like.h5')\n",
        "\n",
        "metadata = _read_metadata(DATASET_PATH)\n",
        "dataset = RiemannH5Dataset(DATASET_PATH)\n",
        "print(f'Dataset contém {len(dataset)} amostras.')\n",
        "\n",
        "cfg = OmegaConf.load(CONFIG_PATH)\n",
        "checkpoint_candidates = sorted(CHECKPOINT_DIR.glob('fno_epoch_*.pt'))\n",
        "if not checkpoint_candidates:\n",
        "    raise FileNotFoundError(f'Sem checkpoints em {CHECKPOINT_DIR}')\n",
        "latest_checkpoint = checkpoint_candidates[-1]\n",
        "print('Usando checkpoint:', latest_checkpoint)\n",
        "\n",
        "model = FNO1DModel(OmegaConf.to_container(cfg.model, resolve=True))\n",
        "device = torch.device(cfg.training.device if torch.cuda.is_available() else 'cpu')\n",
        "state = torch.load(latest_checkpoint, map_location=device)\n",
        "state_dict = state.get('model_state', state)\n",
        "state_dict.pop('_metadata', None)\n",
        "model.load_state_dict(state_dict)\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "sample_idx = int(np.random.randint(0, len(dataset)))\n",
        "inputs, targets = dataset[sample_idx]\n",
        "with torch.no_grad():\n",
        "    preds = model(inputs.unsqueeze(0).to(device)).cpu().numpy()[0]\n",
        "\n",
        "rho_true, u_true, p_true = targets.numpy()\n",
        "rho_pred, u_pred, p_pred = preds\n",
        "x = metadata.x\n",
        "\n",
        "r_l2 = relative_l2(rho_pred, rho_true)\n",
        "u_l2 = relative_l2(u_pred, u_true)\n",
        "p_l2 = relative_l2(p_pred, p_true)\n",
        "print(f'Relative L2 (rho) = {r_l2:.4f}, (u) = {u_l2:.4f}, (p) = {p_l2:.4f}')\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "fig, axes = plt.subplots(3, 1, figsize=(10, 9), sharex=True)\n",
        "axes[0].plot(x, rho_true, label='True')\n",
        "axes[0].plot(x, rho_pred, '--', label='FNO')\n",
        "axes[0].set_ylabel('Density')\n",
        "axes[0].grid(True)\n",
        "axes[0].legend()\n",
        "\n",
        "axes[1].plot(x, u_true, label='True')\n",
        "axes[1].plot(x, u_pred, '--', label='FNO')\n",
        "axes[1].set_ylabel('Velocity')\n",
        "axes[1].grid(True)\n",
        "\n",
        "axes[2].plot(x, p_true, label='True')\n",
        "axes[2].plot(x, p_pred, '--', label='FNO')\n",
        "axes[2].set_ylabel('Pressure')\n",
        "axes[2].set_xlabel('x')\n",
        "axes[2].grid(True)\n",
        "\n",
        "fig.suptitle(f'FNO vs referência (sample {sample_idx})')\n",
        "fig.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Métricas em lote\n\nConsulta dos resultados agregados gerados pelo script de avaliação, úteis para comentar generalização.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import json\n",
        "from pathlib import Path\n",
        "\n",
        "metrics_path = Path('data/artifacts/eval/dataset/dataset_metrics.json')\n",
        "if metrics_path.exists():\n",
        "    metrics = json.loads(metrics_path.read_text(encoding='utf-8'))\n",
        "    print(f'Total de amostras avaliadas: {len(metrics)}')\n",
        "    if metrics:\n",
        "        first = metrics[0]\n",
        "        print('Exemplo de métricas por modelo para a primeira amostra:')\n",
        "        for model_name, values in first['metrics'].items():\n",
        "            print(f'  {model_name}:')\n",
        "            for key, value in values.items():\n",
        "                print(f'    {key}: {value}')\n",
        "else:\n",
        "    print('Arquivo de métricas não encontrado:', metrics_path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "> **Comentário:** use os valores acima para discutir como o FNO performa fora da condição Sod e quais faixas de erro são aceitáveis.\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
